{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy, Percision and Recall\n",
    "\n",
    "Definations:\n",
    "\n",
    "$$\n",
    "\\text{accuracy} = \\frac{\\text{total no. of true predictions}} {\\text{ toatl no. of samples}} = \\frac{\\text{no. of true  positive} + \\text{no. of true  negative}} {\\text{ toatl no. of samples}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\text{precision of positive }= \\frac{\\text{no. of true  positive}} {\\text{no. of predicted positive }}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{recall of positive} = \\frac {\\text{no. of true positive}} {\\text{no. of actual labeled positive}}\n",
    "$$\n",
    "\n",
    "Basically speaking, \n",
    "* Accuracy is telling you generally how well your model is performing\n",
    "* Precision is telling about how precise your model is, or how many of your predictions are really predictions\n",
    "* Recall is describing amoung those labels to be predicted, how many of them you perdicted correctly, or how many of them are found by your predictions.\n",
    "\n",
    "And precision, recall of negative is callculated as:\n",
    "\n",
    "$$\\text{precision of negative }= \\frac{\\text{no. of true  negative}} {\\text{no. of actual labeled negative}}$$\n",
    "\n",
    "$$\\text{recall of negative} = \\frac{\\text{no. of true  negative}} {\\text{no. of actual labeled negative}}$$\n",
    "\n",
    "\n",
    "Let's see a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "Let's say we have a sample base of 10 people and 5 of them having cancer and 5 of them not having cancer. We label having cancer as 1 and not having cancer as 0.\n",
    "\n",
    "* And our model1 predicting 2 person having cancer and they are all correct.\n",
    "* Our model2 predicting 8 people having cancer and all 5 of the true patient is predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n"
     ]
    }
   ],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample =             np.array([1,1,1,1,1, 0,0,0,0,0])\n",
    "model1_predictions = np.array([1,1,0,0,0, 0,0,0,0,0])\n",
    "model2_predictions = np.array([1,1,1,1,1, 1,1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 accuracy is:  0.7\n",
      "Model 1 precision is: 1.0\n",
      "Model 1 recall is:    0.4\n",
      "\n",
      "Model 2 accuracy is:  0.7\n",
      "Model 2 precision is: 0.625\n",
      "Model 2 recall is:    1.0\n"
     ]
    }
   ],
   "source": [
    "model1_acc = sum(sample == model1_predictions) / float(len(sample))\n",
    "model1_precision = sum(sample & model1_predictions) / float(sum(model1_predictions))\n",
    "model1_recall = sum(sample & model1_predictions) / float(sum(sample))\n",
    "\n",
    "model2_acc = sum(sample == model2_predictions) / float(len(sample))\n",
    "model2_precision = sum(sample & model2_predictions) / float(sum(model2_predictions))\n",
    "model2_recall = sum(sample & model2_predictions) / float(sum(sample))\n",
    "\n",
    "print \"Model 1 accuracy is:  {}\".format(model1_acc)\n",
    "print \"Model 1 precision is: {}\".format(model1_precision)\n",
    "print \"Model 1 recall is:    {}\".format(model1_recall)\n",
    "print                                      \n",
    "print \"Model 2 accuracy is:  {}\".format(model2_acc)\n",
    "print \"Model 2 precision is: {}\".format(model2_precision)\n",
    "print \"Model 2 recall is:    {}\".format(model2_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, ultilizing sklear package to perform the same calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 accuracy is:  0.7\n",
      "Model 1 precision is: 1.0\n",
      "Model 1 recall is:    0.4\n",
      "\n",
      "Model 2 accuracy is:  0.7\n",
      "Model 2 precision is: 0.625\n",
      "Model 2 recall is:    1.0\n"
     ]
    }
   ],
   "source": [
    "model1_acc = accuracy_score(sample, model1_predictions)\n",
    "model1_precision = precision_score(sample, model1_predictions)\n",
    "model1_recall = recall_score(sample, model1_predictions)\n",
    "\n",
    "model2_acc = accuracy_score(sample, model2_predictions)\n",
    "model2_precision = precision_score(sample, model2_predictions)\n",
    "model2_recall = recall_score(sample, model2_predictions)\n",
    "\n",
    "print \"Model 1 accuracy is:  {}\".format(model1_acc)\n",
    "print \"Model 1 precision is: {}\".format(model1_precision)\n",
    "print \"Model 1 recall is:    {}\".format(model1_recall)\n",
    "print \n",
    "print \"Model 2 accuracy is:  {}\".format(model2_acc)\n",
    "print \"Model 2 precision is: {}\".format(model2_precision)\n",
    "print \"Model 2 recall is:    {}\".format(model2_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "Accuray is not telling you everything. Similar accuracy models could be very different in their characteristics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "Now let's look like what would happend if the sample is skewed heavily.\n",
    "\n",
    "Let's say we have a sample base of 20 people and 5 of them having cancer and 15 of them not having cancer. We label having cancer as 1 and not having cancer as 0.\n",
    "\n",
    "* And our model1 predicting 6 person having cancer and 4 of them are correct.\n",
    "* Our model2 predicting 2 people having cancer and all 2 of the true patient is predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample =             np.array([1,1,1,1,1, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0])\n",
    "model1_predictions = np.array([1,1,1,1,0, 1,1,0,0,0, 0,0,0,0,0, 0,0,0,0,0])\n",
    "model2_predictions = np.array([1,1,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 accuracy is:  0.85\n",
      "Model 1 precision is: 0.666666666667\n",
      "Model 1 recall is:    0.8\n",
      "\n",
      "Model 2 accuracy is:  0.85\n",
      "Model 2 precision is: 1.0\n",
      "Model 2 recall is:    0.4\n"
     ]
    }
   ],
   "source": [
    "model1_acc = accuracy_score(sample, model1_predictions)\n",
    "model1_precision = precision_score(sample, model1_predictions)\n",
    "model1_recall = recall_score(sample, model1_predictions)\n",
    "\n",
    "model2_acc = accuracy_score(sample, model2_predictions)\n",
    "model2_precision = precision_score(sample, model2_predictions)\n",
    "model2_recall = recall_score(sample, model2_predictions)\n",
    "\n",
    "print \"Model 1 accuracy is:  {}\".format(model1_acc)\n",
    "print \"Model 1 precision is: {}\".format(model1_precision)\n",
    "print \"Model 1 recall is:    {}\".format(model1_recall)\n",
    "print \n",
    "print \"Model 2 accuracy is:  {}\".format(model2_acc)\n",
    "print \"Model 2 precision is: {}\".format(model2_precision)\n",
    "print \"Model 2 recall is:    {}\".format(model2_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to notice here:\n",
    "\n",
    "* Again we have reached to a point were we have the same accuracy for two different model predictions\n",
    "* Model 1 having an okay precision means their cancer prediction is relatively precise, though model 2 has a much higher precision at 1.0, which means all their cancer prediction is correct\n",
    "* However, model 2 has a very low recall, meaning they also it didn't do a good job on finding most of the person who having cancer. With model 2, some people having cancer but predicted no cancer.\n",
    "\n",
    "Thus, for some typical cases, people tend to make a model that give a pretty high precision so we can trust our prediction and in the same time not missing too much target - relative high recall as well. \n",
    "\n",
    "To make it easier, there is another scall called F1 score which is build on precision and recall:\n",
    "\n",
    "$$F_1 = 2 \\frac{PR}{P+R}$$\n",
    "\n",
    "where P stands for precision and R stands for recall.\n",
    "\n",
    "Let's try calculate the F1 score on our example 2 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 accuracy is:  0.727272727273\n",
      "Model 2 accuracy is:  0.571428571429\n"
     ]
    }
   ],
   "source": [
    "model1_f1 = f1_score(sample, model1_predictions)\n",
    "model2_f1 = f1_score(sample, model2_predictions)\n",
    "\n",
    "print \"Model 1 accuracy is:  {}\".format(model1_f1)\n",
    "print \"Model 2 accuracy is:  {}\".format(model2_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this F1 score, we could say generally Model1 is better than Model2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
